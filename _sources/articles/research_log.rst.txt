.. post:: 2019-09-04
    :tags: 
    :author: φKN1GHT
    :exclude:

    这是一篇会定期更新的研究日志，用于记录自己的失败过程和痛苦的经历。一方面用以自诫，另一方面也希望能帮助未来的研究者们少走一些弯路。

研究日志（九月）
================


2019-09-21
----------

在油条的帮助下用再生龙成功实现Intel 600P到SN 500的无痛迁移，更改了缓存结构

加速比达到了 1.28

继续攒钱买内存条

2019-09-20
----------

睡前看到有朋友转发内容

    我没有女人。这世界上有无数的女人，但是一个都不是我的。她们会笑，会骂，会娇嗔，会羞怯——但是不会爱我。

    我没有女人可以抱，我像老鼠一样窃闻她们路过时捎来的香风。我想做一支女人抽的烟，她们脂白细腻的手捏着我，送进丰腴柔软的嘴唇中间。我的欢欣有了实质，我一寸寸碎成灰，魂就爆开了烟，擦过女人的脸颊，黏进她们充斥着洗护品浑厚气味的发云里。

    我需要一个女人，就像冬夜的流浪狗寻一条能裹身的毯子。女人！她们听不见我心里的怒吼仿佛哀嚎一样令人可怜。她们多情又绝情，她们之中不能有一人来属于我。啊！女人！！！！！给老子一个女人！！！！！！

觉得太过矫情，于是工整地仿写了一段

    我没有计算卡。这世界上有无数种型号的计算卡，但是一个我都没有。他们会逻辑运算，整型运算，浮点运算——但都不会被拿来处理我的数据。

    我没有计算卡可以跑程序，我像冬天里的刺猬那样渴望着它们运行时产生的余温。我想成为计算卡中的张量，流淌在那温暖的PCIE总线里，经由数据总线被分割送入一排排算术逻辑单元。于是我便化作一个个细碎的标量，参与进了数值运算的海洋，计算的过程使得我从纯粹的数据变成了意义与结论，而后又略过各级缓存，回到了那广阔无垠的显存空间中。

    我需要一张计算卡，就像失去了时钟信号的控制核心渴求着外部晶振。计算卡！它们看不到我积存的数据已如噪声一般毫无价值。它们精确又冷漠，它们之中不能有一个属于我。啊，计算卡！！！！！给老子一个计算卡！！！！！！

嗯，好像换成这个抒情对象也没什么问题？

2019-09-17
----------

做完周二讲座PPT后开始补之前落下的各种作业

微机原理真是使人头大

2019-09-16
----------

上午接到消息说因为之前有参与科研项目做过贡献

本月还能拿到一点补助

于是非常开心，整个人都精神了不少

下午回到学校

晚上帮着搞运维的朋友面试了一拨人

（随后顺利被安排入IT部编制？似乎可以有个办公位置养老了）

晚课后跑去给新生做蓝桥杯与ICPC入门培训

（被吐糟讲得深+不实用+语速过快）

准备工博会事宜

2019-09-12~2019-09-15
---------------------

快乐建模

2019-09-10~2019-09-11
---------------------

因为感冒很不舒服所以废了两天

期间头脑发热怒买了西数WD Blue SN500（可能还会考虑再买一个）

2019-09-09
----------

新学期第一次大创会议

研究方向被导师驳回

2019-09-08
----------

继续摸鱼的一天

早上八点从床上爬起来准备做网络赛

结果白给 九点半又滚回寝室爬上床补觉

下午网络赛日常自闭

机械硬盘上pkl约80个/s，npy约100个/s，移动到ssd上后1000/s

2019-09-07
----------

摸鱼的一天

上午去帮着收拾了一下嵌入式实验室

下午网络赛日常自闭

晚上和小伙伴们组团打卡了《罗小黑战绩》

2019-09-06
----------

**凌晨**

继续写了一点《基于图卷积网络的深度学习入门》

TODO：拆分成几个章节，补充之后的semi-supervised训练部分

**上午**

完成了之前提到的机甲的布线工作


2019-09-05
----------

**凌晨**

和黎博士在机房里一起研究了半天终于成功使用锐龙3700全核心满速编译出UE4

.. raw:: html

   <s>AMD, YES！</s>

.. raw:: html

   结果发现原来背后的原因是，有个沙雕的编译优化插件<s>不交钱买正版就会压你CPU使用率</s>，遂怒删之

在黎博士的启发下我也突然想到了解决之前文件读不了的问题的思路

.. raw:: html

   于是<s>转手就是一阵代码魔改</s>成功让data_loader动了起来，两个模型的数据集特征提取工作也全部成功完成

原理是把大对象的索引以文件名拆成一个个小文件然后保存在文件夹里

.. raw:: html

   <s>然而试着用通配符mv的时候发现居然爆参数了！于是只能find+管道符一通操作</s>

.. code:: bash

	find data/ -name "*_vfeats*.pkl" | xargs -i mv {} ../vfeats/


然后终于让VQA Challenge 2017 TOP 1的模型成功跑了起来，回去睡觉+挂机等待结果……

TODO: 给视觉特征增加缓存。batch_size为512时训练一个epoch居然要近15分钟，查看资源使用情况发现cpu和gpu使用率都很低，应该是由于视觉特征被我拆成12w+个小文件然后卡IO了……


**中午**

昨晚笔记本带回寝室后没接上电源，结果一夜没动它居然又把电全耗完了

.. raw:: html

	<s>强烈吐槽某巨硬的休眠模式，做得跟没用一样</s>

来到机房查看训练结果，相当惨烈。TODO：反思失败原因。

+------------------------------------------------------------------------------+------------------------------------------------------------------------------------+
| .. image:: ..\\_static\\research_log\\failure_2019_09_05\\accuracy_train.png |   .. image:: ..\\_static\\research_log\\failure_2019_09_05\\accuracy_eval.png      |
+------------------------------------------------------------------------------+------------------------------------------------------------------------------------+
| .. image:: ..\\_static\\research_log\\failure_2019_09_05\\loss_train.png     |   .. image:: ..\\_static\\research_log\\failure_2019_09_05\\loss_eval.png          |
+------------------------------------------------------------------------------+------------------------------------------------------------------------------------+

模型虽然没训练成功，今天的天气还真是出奇的好啊。

**傍晚**

.. raw:: html

	给许兄的<s>强无敌</s>六足机甲装上了灯。很久没焊过什么东西了，手非常生。还有一点没完成，明天上午应该就能po出效果图了。

依然没想到能优化数据集读入的方法，h5py的几种driver都狂吃内存，查到线索似乎numpy.memmap实现了对大文件的访问和缓存机制。那么问题来了，我怎么将数据转成numpy格式的大文件呢……

2019-09-04
----------

拜读了邱锡鹏编写的《神经网络与深度学习》的前两章和全部附录，重温一下数学基础与机器学习的常用符号表达。

完成了习题的warmup部分，然后在第一章实现线性回归的习题里被tensorflow的实现卡了。

另外，整个习题部分错漏百出，目测是临时工编写的？也有可能刚刚才出来还没全部check完成

尝试克隆并构建 https://github.com/peteanderson80/bottom-up-attention 和 https://github.com/markdtw/vqa-winner-cvprw-2017 都失败

trainval_resnet101_faster_rcnn_genome_36.tsv 整个文件有足足45GB 实在是太大了

python读到一半内存占用率超过95.7%然后直接宕机

希望能寻找到能够处理它的方法

或是换个思路……自行提取出特征然后分块做成mmap？

阅读《Learning Visual Knowledge Memory Networks for Visual Question Answering》( http://openaccess.thecvf.com/content_cvpr_2018/html/Su_Learning_Visual_Knowledge_CVPR_2018_paper.html )

依然没有找到较好的融合图像知识（visual knowledge/image knowledge）的方法

希望明天能获得一些收获


