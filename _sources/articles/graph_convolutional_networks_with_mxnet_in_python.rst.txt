.. post:: 2019-8-28
    :tags: GCN
    :author: φKN1GHT
    :exclude:

基于图卷积网络的深度学习入门(MXNET实现)
===============================================

**本文并未完结且大量内容为对原文内容的脑内机翻，请谨慎阅读或是等待笔者补完**

在图结构上的机器学习一直是一个非常难啃的任务，主要由于极高的复杂性和图本身所具有的结构信息。这篇文章的目的是为了简述如何在图卷积网络上实现深度学习。图卷积网络(GCN)是一个强有力的神经网络，被设计为直接在图上，并最大化利用他们的结构信息。

在本文的第一部分，我将会简要介绍GCN的结构并使用样例代码简要说明数据是如何在GCN的隐藏层之间传播的。我们将看到GCN如何聚合来自先前层的信息以及该机制如何在图中生成节点的有用特征表示。

什么是图卷积神经网络？
======================

图卷积网络(GCNs)是一种在图结构上非常有效的神经网络结构。
其实，即是随机初始化的两层GCN结构都能对网络中的结点产生有用的特征表征。
形式化地讲
一个图卷积神经网络是一个在图上进行运算的神经网络。给定图 :math:`G=(V,E)`，一个GCN以如下作为输入：

* 一个输入大小为 :math:`N \times F^0` 的特征矩阵 :math:`X` ，这里 :math:`N` 是结点的数量，而 :math:`F^0` 是每个结点的输入特征。 

* 一个 :math:`N\times N` 大小的邻接矩阵 :math:`A`

因此，GCN中的一个硬汉层可以被写作如下形式：
:math:`H^i=f(H^{i-1},A)`

其中， :math:`H^0=X` 且 :math:`f` 是传播函数。每层 :math:`H^i` 对应一个 :math:`N \times F^i` 的特征矩阵，其中每个行向量都是表征了结点的特征。在每一层，这些特征都被传播函数 :math:`f` 聚合，用于生成下一层的特征。
通过这种方式，特征在每个连接的层中变得越来越抽象。在这个框架中，GCN的变体仅在传播规则 :math:`f` 的选择上有所不同。

一个简单的传播规则
==================

一种最简单的传播规则是：

:math:`f(H^i,A)=\sigma(AH^iW^i)`

这里 :math:`W^i` 是第i层的权重矩阵，而 :math:`\sigma`是一个非线性激活函数，比如ReLU。权值矩阵的维数为 :math:`F^i \time F^{i+1}` 换句话说，权值矩阵的第二个维度的大小决定了下一层的特征数量。如果你熟悉卷积神经网络，这个运算和滤波运算其实十分相似，毕竟权值在所有的结点之间共享。

简化
---------

为了便于理解，我们先从最简单的情况开始检验传播法则的作用。首先，我们令：

* :math:`i=1,\text{s.t.} f` 是一个输入特征矩阵的函数

* :math:`\sigma` 是恒等函数

* 选择权重使得 :math:`AH^0W^0=AXW^0=AX`

换句话说，:math:`f(X,A)=AX` 。这个传播法则也许有一点点太过简单了，但我们将会在稍后逐渐扩充它。另外，稍微提醒一下，AX现在其实和多层感知机的输入等价。

一个简单的图
------------------

我们以一个简单的图结构为例。它的结构如下：

我们试着在Numpy里定义出它的邻接矩阵：
::

    A = np.matrix([
        [0, 1, 0, 0],
        [0, 0, 1, 1], 
        [0, 1, 0, 0],
        [1, 0, 1, 0]],
        dtype=float
    )

接下来，我们需要特征！我们试着为每个结点生成两个整数的特征，基于他们的序号。这让我们容易手动验证矩阵运算的过程。
::

    In [3]: X = np.matrix([[i, -i] for i in range(A.shape[0])], dtype=float)
            X

    Out[3]: matrix([
               [ 0.,  0.],
               [ 1., -1.],
               [ 2., -2.],
               [ 3., -3.]
            ])


应用传播法则
------------------
好的，现在我们有了一个图，它的邻接矩阵A和一个输入的特征X，现在我们试着看看当使用传播法则时会发生什么：

::

    In [6]: A * X
    Out[6]: matrix([
                [ 1., -1.],
                [ 5., -5.],
                [ 1., -1.],
                [ 2., -2.]]

发生了什么？每个结点的特征（即每个行向量）现在变成了它的邻接结点的特征值之和！换句话说，图卷积层将每个结点表示为了其邻居的和。你可以试着自己动手检查一下计算的结果。（注意，对于任一结点，该节点的出度连接的结点被定为其邻居。）

嗯……即将发生的问题出现了
------------------------

你也许已经关注到了一些问题：

* 结点的整合不包括结点自身的特征！既然表征只是结点的邻居的整合，所以只有具有自环的结点会在整合中包括自身的特征。

* 有很多条边的结点会有着巨大的值，而度较小的结点会有着很小的值。这会导致梯度弥散或是梯度消失，对于常用于训练神经网络对数值规模敏感性较强的随机梯度下降算法来说也是病态的。

在接下来的章节中，我们分别来解决这些问题。

增加自环
---------------

为了解决第一个问题，我们可以给每个结点加上一个自环。在实践中，我们通常使邻接矩阵加上单位阵。

::

    In [4]: I = np.matrix(np.eye(A.shape[0]))
            I
    Out[4]: matrix([
                [1., 0., 0., 0.],
                [0., 1., 0., 0.],
                [0., 0., 1., 0.],
                [0., 0., 0., 1.]
            ])

    In [8]: A_hat = A + I
            A_hat * X
    Out[8]: matrix([
                [ 1., -1.],
                [ 6., -6.],
                [ 3., -3.],
                [ 5., -5.]])

现在，结点已经是自己的邻居了，因此结点自身的特征也会被包括进邻居的特征之中了！

正规化特征向量
-------------------------

我们可以对每个结点标准化其特征向量，通过改变将邻接矩阵A乘以他的读矩阵D的逆，因此我们的简单传播法则现在变成了：

.. math ::

	f(X,A)=D^{-1}AXp

让我们看看现在会发生什么。首先我们先计算出度矩阵：

::

    In [9]: D = np.array(np.sum(A, axis=0))[0]
            D = np.matrix(np.diag(D))
            D
    Out[9]: matrix([
                [1., 0., 0., 0.],
                [0., 2., 0., 0.],
                [0., 0., 2., 0.],
                [0., 0., 0., 1.]
            ])

在应用这个法则之前，先让我们看看邻接矩阵在变换后变成了啥样：

**之前**

::

    A = np.matrix([
        [0, 1, 0, 0],
        [0, 0, 1, 1], 
        [0, 1, 0, 0],
        [1, 0, 1, 0]],
        dtype=float
    )

**之后**
::

    In [10]: D**-1 * A
    Out[10]: matrix([
                 [0. , 1. , 0. , 0. ],
                 [0. , 0. , 0.5, 0.5],
                 [0. , 0.5, 0. , 0. ],
                 [0.5, 0. , 0.5, 0. ]
    ])


我们可以观察到邻接矩阵每行的值都被除以该行对应的节点的度。我们现在对变换后的邻接矩阵应用传播法则。

::

    In [11]: D**-1 * A * X
    Out[11]: matrix([
                 [ 1. , -1. ],
                 [ 2.5, -2.5],
                 [ 0.5, -0.5],
                 [ 2. , -2. ]
             ])

于是，我们得到了节点关于相邻节点的表示。这是因为转换后的邻接矩阵的值相当于邻接结点的权值的加权之和。再一次的，我建议你动手进行以下验算。

结合两种方案
=================

我们现在能同时结合自环和正规化。另外，我们要重新介绍以下之前简化了的权重和激活函数。

最先要做的事是计算权重。注意到  :math:`D_hat` 是矩阵 :math:`A_hat=A+I` 的度矩阵，也即强制自环的A的度矩阵 

::

    In [45]: W = np.matrix([
                 [1, -1],
                 [-1, 1]
             ])
             D_hat**-1 * A_hat * X * W
    Out[45]: matrix([
                [ 1., -1.],
                [ 4., -4.],
                [ 2., -2.],
                [ 5., -5.]
            ])

如果我们想要对输出的特征表示进行将为，我们可以较小权值矩阵的大小：

::

    In [46]: W = np.matrix([
                 [1],
                 [-1]
             ])
             D_hat**-1 * A_hat * X * W
    Out[46]: matrix([[1.],
            [4.],
            [2.],
            [5.]]
    )

增加激活函数
---------------

我们选择保存特征的维度并用ReLU函数进行激活。首先，我们定义出激活函数：

::

    relu = lambda x : np.maximum(x, 0)

随后，代入数据

*In [51]:*

::

    W = np.matrix([
            [1, -1],
            [-1, 1]
        ])
        relu(D_hat**-1 * A_hat * X * W)

*Out[51]:*

::

    matrix([[1., 0.],
        [4., 0.],
        [2., 0.],
        [5., 0.]])

你看，一个具有邻接矩阵、输入特征、权重和激活函数的隐含层就这样诞生啦！

回到现实
----------------

现在，我们可以在一个真实的图结构上使用图卷积网络。


Zachary’s Karate Club是一个常用的社交网络数据集。其中，图结点表示karate俱乐部的成员，而边则表示他们之间存在互相关系。当Zachary在karate俱乐部学习的时候，俱乐部的管理员和指导员之间发生了冲突，使得整个俱乐部被分成了两个部分。下面这个图展示了这件事情。





构建网络
----------------

现在让我们构建出图卷及网络。实际上，目前我们并不需要对这个网络进行训练，仅仅是随机地初始化就能产生有用的特征表示。我们会用networkx库来实现。


from networkx import karate_club_graph, to_numpy_matrix
zkc = karate_club_graph()
order = sorted(list(zkc.nodes()))
A = to_numpy_matrix(zkc, nodelist=order)
I = np.eye(zkc.number_of_nodes())
A_hat = A + I
D_hat = np.array(np.sum(A_hat, axis=0))[0]
D_hat = np.matrix(np.diag(D_hat))

接着，我们随机地初始化它的边权。

W_1 = np.random.normal(
    loc=0, scale=1, size=(zkc.number_of_nodes(), 4))
W_2 = np.random.normal(
    loc=0, size=(W_1.shape[1], 2))

把图卷积层堆叠起来。这里我们仅用单位阵作为特征表示。也就是说，我们把每个结点都表示成一个one-hot编码的向量。

def gcn_layer(A_hat, D_hat, X, W):
    return relu(D_hat**-1 * A_hat * X * W)

H_1 = gcn_layer(A_hat, D_hat, I, W_1)
H_2 = gcn_layer(A_hat, D_hat, H_1, W_2)
output = H_2

然后提取特征表示。


feature_representations = {
    node: np.array(output)[node] 
    for node in zkc.nodes()}


看，现在的特征表示能很好地分割Zachary’s karate club。我们甚至还没有开始训练。


注意，这个随机初始化的样例很有可能在x和y轴都给出0，所以多初始化几次才能出现这样的结果。

结论
---------------

在这篇文章里，我们从非常高的角度介绍了图卷积网络，并解释了网络里每一层的每一个结点是怎么用整合其邻居的方式对特征进行表征。我们看到了如何去构建网络，以及这样的网络有多么强大。

在下一部分，我会深入解释其中的技术细节并演示如何实现并训练一个图卷积神经网络，用最新提出的半监督学习的方式。




































