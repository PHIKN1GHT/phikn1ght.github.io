
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" lang="zh">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>CCF ADL104《新一代视觉计算》课程内容回顾（Day1） &#8212; 旅人札记</title>
    <link rel="stylesheet" href="../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../_static/language_data.js"></script>
    <script type="text/javascript" src="../../_static/translations.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="author" title="关于这些文档" href="../../about/" />
    <link rel="index" title="索引" href="../../genindex/" />
    <link rel="search" title="搜索" href="../../search/" />
  
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  
  <link rel="alternate" type="application/atom+xml"  href="../../blog/atom.xml" title="weblog Blog">
  
  
  <link href="True" rel="stylesheet">
  
  <style type="text/css">
    ul.ablog-archive {list-style: none; overflow: auto; margin-left: 0px}
    ul.ablog-archive li {float: left; margin-right: 5px; font-size: 80%}
    ul.postlist a {font-style: italic;}
    ul.postlist-style-disc {list-style-type: disc;}
    ul.postlist-style-none {list-style-type: none;}
    ul.postlist-style-circle {list-style-type: circle;}
  </style>

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="ccf-adl104-day1">
<h1><a class="toc-backref" href="#id23">CCF ADL104《新一代视觉计算》课程内容回顾（Day1）</a><a class="headerlink" href="#ccf-adl104-day1" title="永久链接至标题">¶</a></h1>
<div class="contents topic" id="id1">
<p class="topic-title first">文章目录</p>
<ul class="simple">
<li><a class="reference internal" href="#ccf-adl104-day1" id="id23">CCF ADL104《新一代视觉计算》课程内容回顾（Day1）</a><ul>
<li><a class="reference internal" href="#id2" id="id24">引言</a></li>
<li><a class="reference internal" href="#id4" id="id25">《跨媒体智能：表征、分析与应用》</a><ul>
<li><a class="reference internal" href="#id6" id="id26">研究背景</a><ul>
<li><a class="reference internal" href="#id7" id="id27">存在的问题</a></li>
<li><a class="reference internal" href="#id8" id="id28">研究难点</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id9" id="id29">主要任务</a></li>
<li><a class="reference internal" href="#id10" id="id30">研究思想</a></li>
<li><a class="reference internal" href="#id11" id="id31">研究成果</a><ul>
<li><a class="reference internal" href="#id12" id="id32">1. 细粒度图像分类</a></li>
<li><a class="reference internal" href="#pku-fg-xmedia" id="id33">2. 构建细粒度跨媒体检索数据集和评测基准PKU FG-XMedia</a></li>
<li><a class="reference internal" href="#id13" id="id34">3. 跨媒体检索——跨媒体共享语义空间映射</a></li>
<li><a class="reference internal" href="#id14" id="id35">4. 跨媒体检索——跨媒体非对称关系映射</a></li>
<li><a class="reference internal" href="#id15" id="id36">5. 跨媒体检索——跨媒体模型训练问题</a></li>
<li><a class="reference internal" href="#id16" id="id37">6. 跨媒体检索——跨媒体快速检索</a></li>
<li><a class="reference internal" href="#pku-xmedia" id="id38">7. 构建跨媒体检索数据集PKU XMedia</a></li>
<li><a class="reference internal" href="#id17" id="id39">8. 视频描述生成</a></li>
<li><a class="reference internal" href="#id18" id="id40">9. 文本到图像生成</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#id19" id="id41">《从粗放式到集约式：针对复杂数据的鲁棒深度学习方法》</a><ul>
<li><a class="reference internal" href="#id20" id="id42">总概</a></li>
</ul>
</li>
<li><a class="reference internal" href="#self-paced-learning" id="id43">Self-paced Learning</a></li>
<li><a class="reference internal" href="#id22" id="id44">总结</a></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="id2">
<h2><a class="toc-backref" href="#id24">引言</a><a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h2>
<p>八月末，久违地再一次和 <a class="reference external" href="https://github.com/HicoderDR">&#64;HicoderDR</a> 一起来到了南京城。之前曾两次造访过这里，分别是来参加“南京四校骇客马拉松”和“全国大学生智能互联大赛全国决赛”。十分巧合的是，这两场比赛最后的结局都非常惨淡，只能无奈地抱憾而归。而这一次，则是以求学的心态拜访这里，主要是希望为自己的大学生创新创业计划项目寻求一些研究灵感。在南京逗留的四天，我都下榻在了在信息工程大学地铁站附近的七天优品里。</p>
<p>整整三天上午三小时下午三小时的学术报告，确实十分累人，但收获也非常丰富，接触到了许多十分具有价值的新思想，因而略加整理记录于此，希望能为之后的学术工作提供参考。</p>
</div>
<div class="section" id="id4">
<h2><a class="toc-backref" href="#id25">《跨媒体智能：表征、分析与应用》</a><a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h2>
<p>主讲人：彭宇新（<a class="reference external" href="http://www.icst.pku.edu.cn/mipl/">北京大学 计算机科学技术研究所 多媒体信息处理研究室</a> <a class="reference external" href="https://github.com/PKU-ICST-MIPL">Github主页</a>）</p>
<div class="section" id="id6">
<h3><a class="toc-backref" href="#id26">研究背景</a><a class="headerlink" href="#id6" title="永久链接至标题">¶</a></h3>
<p>据卫报统计，视频、图像等多媒体数据已经占据了大数据超过90%的比重，跨媒体指由图像、视频、文本、音频等形式上多源异构，语义上相互关联的互相融合的媒体形态</p>
<p>传统人工智能：文本推理</p>
<p>实际上：多源信息的共同参与</p>
<div class="section" id="id7">
<h4><a class="toc-backref" href="#id27">存在的问题</a><a class="headerlink" href="#id7" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>语义鸿沟：视频图像的计算机特征表示与人类理解的语义概念不一致</li>
<li>异构鸿沟：视频图像包含的视觉、语言等不同模态信息的特征表示不一致</li>
</ul>
</div>
<div class="section" id="id8">
<h4><a class="toc-backref" href="#id28">研究难点</a><a class="headerlink" href="#id8" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>如何综合利用多模态信息缩短语义鸿沟？</li>
<li>如何实现多模态信息的统一表征和综合利用？</li>
</ul>
</div>
</div>
<div class="section" id="id9">
<h3><a class="toc-backref" href="#id29">主要任务</a><a class="headerlink" href="#id9" title="永久链接至标题">¶</a></h3>
<ul class="simple">
<li>跨媒体统一表征理论和模型</li>
</ul>
<blockquote>
<div><ul class="simple">
<li>通过统一表征映射，将表达相似语义的跨媒体数据映射到同一个空间中，转换为相似的统一表征</li>
</ul>
</div></blockquote>
<ul class="simple">
<li>跨媒体关联理解与深度挖掘</li>
</ul>
<blockquote>
<div><ul class="simple">
<li>挖掘跨媒体知识，补充和拓展传统的基于文本的知识体系，研究跨媒体数据关联与融合方法</li>
<li>如何深入理解跨媒体数据关联，实现相似性计算与知识挖掘，是当前研究的重要挑战</li>
</ul>
</div></blockquote>
<ul class="simple">
<li>跨媒体知识图谱构建与学习</li>
</ul>
<blockquote>
<div><ul class="simple">
<li>当前跨媒体分析方法以数据驱动为主，可解释性与可泛化能力不足；需研究知识驱动的跨媒体分析方法</li>
<li>如何构建跨媒体知识图谱，形成跨媒体知识表达与学习的方法体系，为知识驱动方法提供重要依托</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="id10">
<h3><a class="toc-backref" href="#id30">研究思想</a><a class="headerlink" href="#id10" title="永久链接至标题">¶</a></h3>
<p>知识驱动的跨媒体协同推理</p>
</div>
<div class="section" id="id11">
<h3><a class="toc-backref" href="#id31">研究成果</a><a class="headerlink" href="#id11" title="永久链接至标题">¶</a></h3>
<div class="section" id="id12">
<h4><a class="toc-backref" href="#id32">1. 细粒度图像分类</a><a class="headerlink" href="#id12" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>The Application of Two-level Attention Models in Deep Convolutional Neural Network for Fine-grained Image Classification</li>
<li>Object-Part Attention Model for Fine-grained Image Classification</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">为了减少标注依赖，提出空间拓扑注意力学习方法，仅使用图像级标注信息，通过卷积激励的显著分布估计与拓扑关联约束的语义对齐，自动定位对象、部件显著区域，实现图像细粒度辨识与分类</p>
</div>
<ul class="simple">
<li>Only Learn One Sample: Fine-Grained Visual Categorization with One Sample Training</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">针对一个训练样本条件下的图像细分类，提出了基于选择与生成的数据增广方法，通过多示例学习与对抗生成，对数据进行分割、过滤、再选择和生成，实现图像数据的多样性扩增</p>
</div>
<ul class="simple">
<li>Which and How Many Regions to Gaze: Focus Discriminative Regions for Fine-grained Visual Categorization</li>
<li>StackDRL: Stacked Deep Reinforcement Learning for Fine-grained Visual Categorization</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">为了提升图像细分类准确率，提出多尺度堆叠式深度强化学习方法，序列式地定位不同尺度的对象及其显著区域，并自动选择显著区域的数目，避免现有方法依赖人工先验和实验调参造成的扩展性上的局限</p>
</div>
<ul class="simple">
<li>Fine-grained Discriminative Localization via Saliency-guided Faster R-CNN</li>
<li>Fast Fine-grained Image Classification via Weakly Supervised Discriminative Localization</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">针对图像细分类的速度问题，提出了多级注意力引导的快速辨识定位方法，通过多级注意力提取网络与辨识性定位网络，实现了细粒度分类与辨识性区域定位的协同促进，在提升分类准确率的同时，实现分类加速</p>
</div>
<ul class="simple">
<li>Error-Driven Incremental Learning in Deep Convolutional Neural Network for Large-Scale Image Classification</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">针对新增类别训练加速问题，提出层次化深度增量迁移学习方法，通过类别划分的层次化模型与特征迁移的增量训练，对模型进行动态扩容与知识迁移，解决了新增类别训练加速的难题</p>
</div>
<ul class="simple">
<li>Fine-grained Image Classification via Combining Vision and Language</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">提出多源语义嵌入的视觉表示方法, 通过显著协同优化的视觉分支和卷积序列编码的文本分支，挖掘图像文本的语义关联和嵌入表示学习，突破单源信息表示的局限性</p>
</div>
</div>
<div class="section" id="pku-fg-xmedia">
<h4><a class="toc-backref" href="#id33">2. 构建细粒度跨媒体检索数据集和评测基准PKU FG-XMedia</a><a class="headerlink" href="#pku-fg-xmedia" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>A New Benchmark and Approach for Fine-grained Cross-media Retrieval</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">首个包含多达4种媒体类型（图像、文本、视频、音频）的细粒度跨媒体检索公开数据集和评测基准，涵盖200个细粒度类别（鸟大类下的200个子类，如灰翅鸥、灰背鸥、加州海鸥、黑背鸥等）。</p>
</div>
</div>
<div class="section" id="id13">
<h4><a class="toc-backref" href="#id34">3. 跨媒体检索——跨媒体共享语义空间映射</a><a class="headerlink" href="#id13" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>Semi-Supervised Cross-Media Feature Learning with Unified Patch Graph Regularization</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">针对多种媒体的统一表征问题，提出图规约共享语义空间映射方法，建立跨媒体统一关联超图模型，将多种媒体的全局、局部信息以及它们之间的复杂关联关系建模在一个超图中，同时学习多种媒体的统一表征</p>
</div>
<ul class="simple">
<li>CM-GANs: Cross-modal Generative Adversarial Networks for Common Representation Learning</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">针对深度跨媒体统一表征学习，提出了跨媒体生成式对抗网络，构建跨媒体卷积自编码器，通过媒体内和媒体间的对抗训练拟合不同媒体数据的联合分布，提升了跨媒体关联学习的效果</p>
</div>
<ul class="simple">
<li>Cross-media Shared Representation by Hierarchical Learning with Multiple Deep Networks</li>
<li>CCL: Cross-modal Correlation Learning with Multi-grained Fusion by Hierarchical Network</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">针对深度跨媒体细粒度建模问题，提出层叠式耦合关联学习方法，有效融合粗、细粒度的语义表示，动态平衡媒体内与媒体间的关联关系，提高检索准确率</p>
</div>
<ul class="simple">
<li>Cross-media Multi-level Alignment with Relation Attention Network</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">针对细粒度局部关系建模，提出视觉-语言关系注意力模型，建立多级对齐网络，同时建模图像和文本之间全局、局部以及局部关系三个级别的语义对齐，有效促进跨媒体关联检索效果</p>
</div>
</div>
<div class="section" id="id14">
<h4><a class="toc-backref" href="#id35">4. 跨媒体检索——跨媒体非对称关系映射</a><a class="headerlink" href="#id14" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>Modality-specific Cross-modal Similarity Measurement with Recurrent Attention Network</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">针对视觉-语言之间信息不对等问题，提出了特定媒体语义空间映射方法，充分学习不同媒体之间不平衡的关联信息，有效挖掘不同媒体语义空间的互补性，提高了跨媒体关联检索效果</p>
</div>
<ul class="simple">
<li>Cross-modal Bidirectional Translation via Reinforcement Learning</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">针对视觉-语言信息的相互转换问题，提出了跨媒体双向翻译模型，借鉴机器翻译的思想并建立跨媒体强化学习策略，通过媒体间和媒体内两种奖励的相互促进，提升跨媒体关联学习效果</p>
</div>
<ul class="simple">
<li>Show and Tell in the Loop: Cross-Modal Circular Correlation Learning</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">针对视觉-语言信息相互生成问题，提出了跨媒体循环关联学习方法，同时进行图像-文本相互生成及统一表征，通过循环训练及数据增广使不同任务相互促进，提升跨媒体关联学习效果</p>
</div>
</div>
<div class="section" id="id15">
<h4><a class="toc-backref" href="#id36">5. 跨媒体检索——跨媒体模型训练问题</a><a class="headerlink" href="#id15" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>MHTN: Modal-adversarial Hybrid Transfer Network for Cross-modal Retrieval</li>
<li>Deep Cross-media Knowledge Transfer</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">针对跨媒体数据标注成本大的问题，提出混合知识迁移方法，通过模态共享的混合迁移网络与模态对抗的表征学习策略，仅使用包含单一媒体的源域数据，支持跨媒体目标域的模型训练</p>
</div>
<ul class="simple">
<li>Life-long Cross-media Correlation Learning</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">针对新增跨媒体数据的模型训练问题，提出了跨媒体终身学习方法，建立高层知识共享及自适应网络扩容机制，通过域内分布对齐和域间知识蒸馏，利用知识迁移促进新增数据关联学习</p>
</div>
<ul class="simple">
<li>Zero-shot Cross-media Embedding Learning with Dual Adversarial Distribution Network</li>
<li>Dual Adversarial Networks for Zero-shot Cross-media Retrieval</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">针对新增未知数据的检索问题，提出零样本跨媒体对偶对抗学习方法，通过生成对抗网络构成对偶结构以挖掘数据潜在结构信息，利用类别词嵌入进行知识迁移，实现零样本跨媒体检索</p>
</div>
</div>
<div class="section" id="id16">
<h4><a class="toc-backref" href="#id37">6. 跨媒体检索——跨媒体快速检索</a><a class="headerlink" href="#id16" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>Multi-Scale Correlation for Sequential Cross-modal Hashing Learning</li>
<li>Sequential Cross-Modal Hashing Learning via Multi-scale Correlation Mining</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">针对有监督条件下哈希码鲁棒性问题，提出序列化多尺度特征哈希方法，通过多尺度特征指导学习和尺度间关联挖掘，提高了哈希码的多样性和鲁棒性，实现更好的检索结果</p>
</div>
<ul class="simple">
<li>SCH-GAN: Semi-supervised Cross-modal Hashing by Generative Adversarial Network</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">针对半监督条件下的哈希码生成问题，提出半监督跨媒体生成对抗哈希方法：通过生成对抗网络用无标注数据构造边界样本以增强训练；通过离散策略梯度优化生成器实现高效哈希检索</p>
</div>
<ul class="simple">
<li>Unsupervised Generative Adversarial Cross-modal Hashing</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">针对无监督条件下的哈希码生成问题，提出无监督跨媒体生成对抗哈希方法：通过关联图模型建模数据流形结构，利用流形结构上的潜在关联，实现无监督的高效哈希检索</p>
</div>
</div>
<div class="section" id="pku-xmedia">
<h4><a class="toc-backref" href="#id38">7. 构建跨媒体检索数据集PKU XMedia</a><a class="headerlink" href="#pku-xmedia" title="永久链接至标题">¶</a></h4>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<ul class="last simple">
<li><strong>媒体多样</strong>：5种媒体类型（图像、文本、视频、音频、3D）。</li>
<li><strong>数据量大</strong>：超过10万标注数据，200个语义类别，基于wordNet层次结构。</li>
<li><strong>语义明确</strong>：语义类别为具体的物体（如Dog、Airplane等），避免语义混淆。</li>
<li><strong>来源权威</strong>：数据来自著名网站如Wikipedia, Flickr, Youtube, Findsounds, Freesound, Yobi3D同时具备数据量、媒体多样性上的优势，能够全面评测跨媒体相关方法在实际大规模、多样化数据条件下的性能。</li>
</ul>
</div>
</div>
<div class="section" id="id17">
<h4><a class="toc-backref" href="#id39">8. 视频描述生成</a><a class="headerlink" href="#id17" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>Object-aware Aggregation with Bidirectional Temporal Graph for Video Captioning</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">针对视频细粒度时空信息建模问题，提出对象感知双向图方法，从正时序和逆时序方向建立互补的双向时序图，通过建模多个对象动态时序轨迹，生成描述对象时序演化的自然语言文本</p>
</div>
<ul class="simple">
<li>Hierarchical Vision-Language Alignment for Video Captioning</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">针对视频和文本语句语义一致性建模问题，提出层次化视觉-语言对齐方法，通过二元记忆循环网络利用视觉内容与本文语句之间不同粒度的隐含对齐关系，指导生成准确的视频文本描述</p>
</div>
</div>
<div class="section" id="id18">
<h4><a class="toc-backref" href="#id40">9. 文本到图像生成</a><a class="headerlink" href="#id18" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>Text-to-image Synthesis via Symmetrical Distillation Networks</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">提出对称蒸馏网络方法，通过蒸馏学习机制，将知识从通用判别网络迁移到具有对称结构的图像生成网络中，建立文本空间到图像空间的映射，生成与文本内容相符的图像</p>
</div>
</div>
</div>
</div>
<div class="section" id="id19">
<h2><a class="toc-backref" href="#id41">《从粗放式到集约式：针对复杂数据的鲁棒深度学习方法》</a><a class="headerlink" href="#id19" title="永久链接至标题">¶</a></h2>
<div class="section" id="id20">
<h3><a class="toc-backref" href="#id42">总概</a><a class="headerlink" href="#id20" title="永久链接至标题">¶</a></h3>
<p>主讲人：孟德宇（西安交通大学 数学与统计学院 信息科学系 <a class="reference external" href="http://dymeng.gr.xjtu.edu.cn">个人主页</a> ）</p>
<p>机器学习的基本任务：预测</p>
<p>目标：获得预测函数 <span class="math notranslate nohighlight">\(Y=F_W(X)\)</span></p>
<p>传统机器学习方法：模型驱动</p>
<p>现代机器学习方法：数据驱动</p>
<p>大数据带来的问题：本质决策函数规律极其复杂，无法刻画输入输出规律</p>
<p>主讲内容：</p>
<ul class="simple">
<li>自步学习：针对标记噪声模型驱动</li>
<li>元学习：针对各类数据偏差情形模型驱动+数据驱动</li>
<li>噪声建模：拟合噪声分布模型驱动</li>
<li>噪声推断：自适应推断噪声分布模型驱动+数据驱动</li>
</ul>
</div>
</div>
<div class="section" id="self-paced-learning">
<h2><a class="toc-backref" href="#id43">Self-paced Learning</a><a class="headerlink" href="#self-paced-learning" title="永久链接至标题">¶</a></h2>
<p>A simple idea: pseudo-label the data and then feed them back into training</p>
<p>Favoring high-confidence samples: Only feedback some high-confidence samples</p>
<p>From less to more: With the iteration, more samples can be added</p>
<div class="math notranslate nohighlight">
\[min\limits_wv\]</div>
<p>min┬(w,v∈[0,1]^n )⁡∑_(i=1)^n▒v_i  L(f(x_i;w),y_i )+γg(w)-λ‖v‖_1</p>
</div>
<div class="section" id="id22">
<h2><a class="toc-backref" href="#id44">总结</a><a class="headerlink" href="#id22" title="永久链接至标题">¶</a></h2>
</div>
</div>

  <div class="section">
  
    


<div class="section">
  <span style="float: left;">
  
  
  <a href="../graph_convolutional_networks_with_mxnet_in_python/">
    <i class="fa fa-arrow-circle-left"></i>
    基于图卷积网络的深度学习入门(MXNET实现)
  </a>
  
  </span>
  <span>&nbsp;</span>
  <span style="float: right;">
  
  
  <a href="../solution_to_2018_icpc_nanjing_online/">
    ICPC2018南京赛区网络预赛题解
    <i class="fa fa-arrow-circle-right"></i>
  </a>
  </span>
  
</div>

  
  
  </div>

          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../">旅人札记</a></h1>









  
  
  <h2>
  
  <i class="fa fa-calendar"></i>
    2019年8月30日 
  
  </h2>

  <ul>
    

  
  <li id="author"><span><i class="fa-fw fa fa-user"></i></span>
    
      
      φKN1GHT
      
    </li>
  

  

  

  

  
  <li id="tags"><span><i class="fa-fw fa fa-tag"></i></span>
    
      
      <a href="../../blog/tag/icpc/">ICPC</a>
      
    </li>
  
  
  </ul>


<h3>导航</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../about/">关于</a></li>
<li class="toctree-l1"><a class="reference external" href="/blog/#://">所有文章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../indexes/academic_reading/">文献导读索引</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../indexes/knowledge_base/">知识体系索引</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../indexes/academic_review/">学术评论索引</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../indexes/selected_article/">杂文选集</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../indexes/private_library/">收藏馆</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../timestamp/">时间锚</a></li>
</ul>


  <h3><a href="../../blog/tag/">标签</a></h3>
  <style type="text/css">
    ul.ablog-cloud {list-style: none; overflow: auto;}
    ul.ablog-cloud li {float: left; height: 20pt; line-height: 18pt; margin-right: 5px;}
    ul.ablog-cloud a {text-decoration: none; vertical-align: middle;}
    li.ablog-cloud-1{font-size: 80%;}
    li.ablog-cloud-2{font-size: 95%;}
    li.ablog-cloud-3{font-size: 110%;}
    li.ablog-cloud-4{font-size: 125%;}
    li.ablog-cloud-5{font-size: 140%;}
  </style>
  <ul class="ablog-cloud">
    
      
      <li class="ablog-cloud ablog-cloud-1">
        <a href="../../blog/tag/gcn/">GCN</a></li>
      
    
      
      <li class="ablog-cloud ablog-cloud-5">
        <a href="../../blog/tag/icpc/">ICPC</a></li>
      
    
      
      <li class="ablog-cloud ablog-cloud-1">
        <a href="../../blog/tag/number-theory/">Number theory</a></li>
      
    
      
      <li class="ablog-cloud ablog-cloud-1">
        <a href="../../blog/tag/文献导读/">文献导读</a></li>
      
    
      
      <li class="ablog-cloud ablog-cloud-1">
        <a href="../../blog/tag/知识库/">知识库</a></li>
      
    
      
      <li class="ablog-cloud ablog-cloud-1">
        <a href="../../blog/tag/视觉问答/">视觉问答</a></li>
      
    
  </ul>

  <h3><a href="../../blog/archive/">归档</a></h3>
  <ul>
  
    
    <li><a href="../../blog/2019/">2019 (7)</a></li>
    
  
  </ul>

<div id="searchbox" style="display: none" role="search">
  <h3>快速搜索</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search/" method="get">
      <input type="text" name="q" />
      <input type="submit" value="转向" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, φKN1GHT.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.8.5</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../../_sources/articles/review_of_adl104_day1.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>