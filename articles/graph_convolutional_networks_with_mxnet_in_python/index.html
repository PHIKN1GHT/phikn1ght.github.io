
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" lang="zh">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>基于图卷积网络的深度学习入门(MXNET实现) &#8212; 旅人札记</title>
    <link rel="stylesheet" href="../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../_static/language_data.js"></script>
    <script type="text/javascript" src="../../_static/translations.js"></script>
    <link rel="author" title="关于这些文档" href="../../about/" />
    <link rel="index" title="索引" href="../../genindex/" />
    <link rel="search" title="搜索" href="../../search/" />
  
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  
  <link rel="alternate" type="application/atom+xml"  href="../../blog/atom.xml" title="weblog Blog">
  
  
  <link href="True" rel="stylesheet">
  
  <style type="text/css">
    ul.ablog-archive {list-style: none; overflow: auto; margin-left: 0px}
    ul.ablog-archive li {float: left; margin-right: 5px; font-size: 80%}
    ul.postlist a {font-style: italic;}
    ul.postlist-style-disc {list-style-type: disc;}
    ul.postlist-style-none {list-style-type: none;}
    ul.postlist-style-circle {list-style-type: circle;}
  </style>

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="mxnet">
<h1>基于图卷积网络的深度学习入门(MXNET实现)<a class="headerlink" href="#mxnet" title="永久链接至标题">¶</a></h1>
<p>在图结构上的机器学习一直是一个非常难啃的任务，主要由于极高的复杂性和图本身所具有的结构信息。这篇文章的目的是为了简述如何在图卷积网络上实现深度学习。图卷积网络(GCN)是一个强有力的神经网络，被设计为直接在图上，并最大化利用他们的结构信息。</p>
<p>在本文的第一部分，我将会简要介绍GCN的结构并使用样例代码简要说明数据是如何在GCN的隐藏层之间传播的。我们将看到GCN如何聚合来自先前层的信息以及该机制如何在图中生成节点的有用特征表示。</p>
</div>
<div class="section" id="id1">
<h1>什么是图卷积神经网络？<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h1>
<p>图卷积网络(GCNs)是一种在图结构上非常有效的神经网络结构。
其实，即是随机初始化的两层GCN结构都能对网络中的结点产生有用的特征表征。
形式化地讲
一个图卷积神经网络是一个在图上进行运算的神经网络。给定图 <span class="math notranslate nohighlight">\(G=(V,E)\)</span>，一个GCN以如下作为输入：</p>
<ul class="simple">
<li>一个输入大小为 <span class="math notranslate nohighlight">\(N \times F^0\)</span> 的特征矩阵 <span class="math notranslate nohighlight">\(X\)</span> ，这里 <span class="math notranslate nohighlight">\(N\)</span> 是结点的数量，而 <span class="math notranslate nohighlight">\(F^0\)</span> 是每个结点的输入特征。</li>
<li>一个 <span class="math notranslate nohighlight">\(N\times N\)</span> 大小的邻接矩阵 <span class="math notranslate nohighlight">\(A\)</span></li>
</ul>
<p>因此，GCN中的一个硬汉层可以被写作如下形式：
<span class="math notranslate nohighlight">\(H^i=f(H^{i-1},A)\)</span></p>
<p>其中， <span class="math notranslate nohighlight">\(H^0=X\)</span> 且 <span class="math notranslate nohighlight">\(f\)</span> 是传播函数。每层 <span class="math notranslate nohighlight">\(H^i\)</span> 对应一个 <span class="math notranslate nohighlight">\(N \times F^i\)</span> 的特征矩阵，其中每个行向量都是表征了结点的特征。在每一层，这些特征都被传播函数 <span class="math notranslate nohighlight">\(f\)</span> 聚合，用于生成下一层的特征。
通过这种方式，特征在每个连接的层中变得越来越抽象。在这个框架中，GCN的变体仅在传播规则 <span class="math notranslate nohighlight">\(f\)</span> 的选择上有所不同。</p>
</div>
<div class="section" id="id2">
<h1>一个简单的传播规则<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h1>
<p>一种最简单的传播规则是：</p>
<p><span class="math notranslate nohighlight">\(f(H^i,A)=\sigma(AH^iW^i)\)</span></p>
<p>这里 <span class="math notranslate nohighlight">\(W^i\)</span> 是第i层的权重矩阵，而 <span class="math notranslate nohighlight">\(\sigma\)</span> 换句话说，权值矩阵的第二个维度的大小决定了下一层的特征数量。如果你熟悉卷积神经网络，这个运算和滤波运算其实十分相似，毕竟权值在所有的结点之间共享。</p>
<div class="section" id="id3">
<h2>简化<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h2>
<p>为了便于理解，我们先从最简单的情况开始检验传播法则的作用。首先，我们令：</p>
<ul class="simple">
<li><span class="math notranslate nohighlight">\(i=1,\text{s.t.} f\)</span> 是一个输入特征矩阵的函数</li>
<li><span class="math notranslate nohighlight">\(\sigma\)</span> 是恒等函数</li>
<li>选择权重使得 <span class="math notranslate nohighlight">\(AH^0W^0=AXW^0=AX\)</span></li>
</ul>
<p>换句话说，<span class="math notranslate nohighlight">\(f(X,A)=AX\)</span> 。这个传播法则也许有一点点太过简单了，但我们将会在稍后逐渐扩充它。另外，稍微提醒一下，AX现在其实和多层感知机的输入等价。</p>
</div>
<div class="section" id="id4">
<h2>一个简单的图<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h2>
<p>我们以一个简单的图结构为例。它的结构如下：</p>
<p>我们试着在Numpy里定义出它的邻接矩阵：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span>
    <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span>
<span class="p">)</span>
</pre></div>
</div>
<p>接下来，我们需要特征！我们试着为每个结点生成两个整数的特征，基于他们的序号。这让我们容易手动验证矩阵运算的过程。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">In</span> <span class="p">[</span><span class="mi">3</span><span class="p">]:</span> <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([[</span><span class="n">i</span><span class="p">,</span> <span class="o">-</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
        <span class="n">X</span>

<span class="n">Out</span><span class="p">[</span><span class="mi">3</span><span class="p">]:</span> <span class="n">matrix</span><span class="p">([</span>
           <span class="p">[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">],</span>
           <span class="p">[</span> <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">],</span>
           <span class="p">[</span> <span class="mf">2.</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">],</span>
           <span class="p">[</span> <span class="mf">3.</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.</span><span class="p">]</span>
        <span class="p">])</span>
</pre></div>
</div>
</div>
<div class="section" id="id5">
<h2>应用传播法则<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h2>
<p>好的，现在我们有了一个图，它的邻接矩阵A和一个输入的特征X，现在我们试着看看当使用传播法则时会发生什么：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">In</span> <span class="p">[</span><span class="mi">6</span><span class="p">]:</span> <span class="n">A</span> <span class="o">*</span> <span class="n">X</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">6</span><span class="p">]:</span> <span class="n">matrix</span><span class="p">([</span>
            <span class="p">[</span> <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">],</span>
            <span class="p">[</span> <span class="mf">5.</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.</span><span class="p">],</span>
            <span class="p">[</span> <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">],</span>
            <span class="p">[</span> <span class="mf">2.</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">]]</span>
</pre></div>
</div>
<p>发生了什么？每个结点的特征（即每个行向量）现在变成了它的邻接结点的特征值之和！换句话说，图卷积层将每个结点表示为了其邻居的和。你可以试着自己动手检查一下计算的结果。（注意，对于任一结点，该节点的出度连接的结点被定为其邻居。）</p>
</div>
<div class="section" id="id6">
<h2>嗯……即将发生的问题出现了<a class="headerlink" href="#id6" title="永久链接至标题">¶</a></h2>
<p>你也许已经关注到了一些问题：</p>
<ul class="simple">
<li>结点的整合不包括结点自身的特征！既然表征只是结点的邻居的整合，所以只有具有自环的结点会在整合中包括自身的特征。</li>
<li>有很多条边的结点会有着巨大的值，而度较小的结点会有着很小的值。这会导致梯度弥散或是梯度消失，对于常用于训练神经网络对数值规模敏感性较强的随机梯度下降算法来说也是病态的。</li>
</ul>
<p>在接下来的章节中，我们分别来解决这些问题。</p>
</div>
<div class="section" id="id7">
<h2>增加自环<a class="headerlink" href="#id7" title="永久链接至标题">¶</a></h2>
<p>为了解决第一个问题，我们可以给每个结点加上一个自环。在实践中，我们通常使邻接矩阵加上单位阵。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">In</span> <span class="p">[</span><span class="mi">4</span><span class="p">]:</span> <span class="n">I</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">I</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">4</span><span class="p">]:</span> <span class="n">matrix</span><span class="p">([</span>
            <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
            <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
            <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
            <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]</span>
        <span class="p">])</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">8</span><span class="p">]:</span> <span class="n">A_hat</span> <span class="o">=</span> <span class="n">A</span> <span class="o">+</span> <span class="n">I</span>
        <span class="n">A_hat</span> <span class="o">*</span> <span class="n">X</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">8</span><span class="p">]:</span> <span class="n">matrix</span><span class="p">([</span>
            <span class="p">[</span> <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">],</span>
            <span class="p">[</span> <span class="mf">6.</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.</span><span class="p">],</span>
            <span class="p">[</span> <span class="mf">3.</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.</span><span class="p">],</span>
            <span class="p">[</span> <span class="mf">5.</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.</span><span class="p">]])</span>
</pre></div>
</div>
<p>现在，结点已经是自己的邻居了，因此结点自身的特征也会被包括进邻居的特征之中了！</p>
</div>
<div class="section" id="id8">
<h2>正规化特征向量<a class="headerlink" href="#id8" title="永久链接至标题">¶</a></h2>
<p>我们可以对每个结点标准化其特征向量，通过改变将邻接矩阵A乘以他的读矩阵D的逆，因此我们的简单传播法则现在变成了：</p>
<div class="math notranslate nohighlight">
\[f(X,A)=D^{-1}AX\]</div>
<p>让我们看看现在会发生什么。首先我们先计算出度矩阵：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">In</span> <span class="p">[</span><span class="mi">9</span><span class="p">]:</span> <span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">D</span><span class="p">))</span>
        <span class="n">D</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">9</span><span class="p">]:</span> <span class="n">matrix</span><span class="p">([</span>
            <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
            <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
            <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
            <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]</span>
        <span class="p">])</span>
</pre></div>
</div>
<p>在应用这个法则之前，先让我们看看邻接矩阵在变换后变成了啥样：</p>
<p><strong>之前</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span>
    <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>之后</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">In</span> <span class="p">[</span><span class="mi">10</span><span class="p">]:</span> <span class="n">D</span><span class="o">**-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">A</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">10</span><span class="p">]:</span> <span class="n">matrix</span><span class="p">([</span>
             <span class="p">[</span><span class="mf">0.</span> <span class="p">,</span> <span class="mf">1.</span> <span class="p">,</span> <span class="mf">0.</span> <span class="p">,</span> <span class="mf">0.</span> <span class="p">],</span>
             <span class="p">[</span><span class="mf">0.</span> <span class="p">,</span> <span class="mf">0.</span> <span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
             <span class="p">[</span><span class="mf">0.</span> <span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.</span> <span class="p">,</span> <span class="mf">0.</span> <span class="p">],</span>
             <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.</span> <span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.</span> <span class="p">]</span>
<span class="p">])</span>
</pre></div>
</div>
<p>我们可以观察到邻接矩阵每行的值都被除以该行对应的节点的度。我们现在对变换后的邻接矩阵应用传播法则。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">In</span> <span class="p">[</span><span class="mi">11</span><span class="p">]:</span> <span class="n">D</span><span class="o">**-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">A</span> <span class="o">*</span> <span class="n">X</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">11</span><span class="p">]:</span> <span class="n">matrix</span><span class="p">([</span>
             <span class="p">[</span> <span class="mf">1.</span> <span class="p">,</span> <span class="o">-</span><span class="mf">1.</span> <span class="p">],</span>
             <span class="p">[</span> <span class="mf">2.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5</span><span class="p">],</span>
             <span class="p">[</span> <span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">],</span>
             <span class="p">[</span> <span class="mf">2.</span> <span class="p">,</span> <span class="o">-</span><span class="mf">2.</span> <span class="p">]</span>
         <span class="p">])</span>
</pre></div>
</div>
<p>于是，我们得到了节点关于相邻节点的表示。这是因为转换后的邻接矩阵的值相当于邻接结点的权值的加权之和。再一次的，我建议你动手进行以下验算。</p>
</div>
</div>
<div class="section" id="id9">
<h1>结合两种方案<a class="headerlink" href="#id9" title="永久链接至标题">¶</a></h1>
<p>我们现在能同时结合自环和正规化。另外，我们要重新介绍以下之前简化了的权重和激活函数。</p>
<p>最先要做的事是计算权重。注意到  <span class="math notranslate nohighlight">\(D_hat\)</span> 是矩阵 <span class="math notranslate nohighlight">\(A_hat=A+I\)</span> 的度矩阵，也即强制自环的A的度矩阵</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">In</span> <span class="p">[</span><span class="mi">45</span><span class="p">]:</span> <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
             <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
             <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
         <span class="p">])</span>
         <span class="n">D_hat</span><span class="o">**-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">A_hat</span> <span class="o">*</span> <span class="n">X</span> <span class="o">*</span> <span class="n">W</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">45</span><span class="p">]:</span> <span class="n">matrix</span><span class="p">([</span>
            <span class="p">[</span> <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">],</span>
            <span class="p">[</span> <span class="mf">4.</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.</span><span class="p">],</span>
            <span class="p">[</span> <span class="mf">2.</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">],</span>
            <span class="p">[</span> <span class="mf">5.</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.</span><span class="p">]</span>
        <span class="p">])</span>
</pre></div>
</div>
<p>如果我们想要对输出的特征表示进行将为，我们可以较小权值矩阵的大小：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">In</span> <span class="p">[</span><span class="mi">46</span><span class="p">]:</span> <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
             <span class="p">[</span><span class="mi">1</span><span class="p">],</span>
             <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
         <span class="p">])</span>
         <span class="n">D_hat</span><span class="o">**-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">A_hat</span> <span class="o">*</span> <span class="n">X</span> <span class="o">*</span> <span class="n">W</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">46</span><span class="p">]:</span> <span class="n">matrix</span><span class="p">([[</span><span class="mf">1.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">4.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">2.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">5.</span><span class="p">]]</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="section" id="id10">
<h2>增加激活函数<a class="headerlink" href="#id10" title="永久链接至标题">¶</a></h2>
<p>我们选择保存特征的维度并用ReLU函数进行激活。首先，我们定义出激活函数：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">relu</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>随后，代入数据</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">In</span> <span class="p">[</span><span class="mi">51</span><span class="p">]:</span> <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
             <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
             <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
         <span class="p">])</span>
         <span class="n">relu</span><span class="p">(</span><span class="n">D_hat</span><span class="o">**-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">A_hat</span> <span class="o">*</span> <span class="n">X</span> <span class="o">*</span> <span class="n">W</span><span class="p">)</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">51</span><span class="p">]:</span> <span class="n">matrix</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]])</span>
</pre></div>
</div>
<p>你看，一个具有邻接矩阵、输入特征、权重和激活函数的隐含层就这样诞生啦！</p>
</div>
</div>

  <div class="section">
  
    


<div class="section">
  <span style="float: left;">
  
  
  <a href="../../academic_reading/explicit_knowledge_based_reasoning_for_visual_question_answering/">
    <i class="fa fa-arrow-circle-left"></i>
    针对视觉问答的基于知识库的显式推理
  </a>
  
  </span>
  <span>&nbsp;</span>
  <span style="float: right;">
  
  
  <a href="../review_of_adl104/">
    CCF ADL104《新一代视觉计算》课程内容回顾
    <i class="fa fa-arrow-circle-right"></i>
  </a>
  </span>
  
</div>

  
  
  </div>

          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../">旅人札记</a></h1>









  
  
  <h2>
  
  <i class="fa fa-calendar"></i>
    2019年8月28日 
  
  </h2>

  <ul>
    

  
  <li id="author"><span><i class="fa-fw fa fa-user"></i></span>
    
      
      φKN1GHT
      
    </li>
  

  

  

  

  
  <li id="tags"><span><i class="fa-fw fa fa-tag"></i></span>
    
      
      <a href="../../blog/tag/gcn/">GCN</a>
      
    </li>
  
  
  </ul>


<h3>导航</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../about/">关于</a></li>
<li class="toctree-l1"><a class="reference external" href="/blog/#://">所有文章</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../indexes/academic_reading/">文献导读索引</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../indexes/knowledge_base/">知识体系索引</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../indexes/academic_review/">学术评论索引</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../indexes/selected_article/">杂文选集</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../indexes/private_library/">收藏馆</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../timestamp/">时间锚</a></li>
</ul>


  <h3><a href="../../blog/tag/">标签</a></h3>
  <style type="text/css">
    ul.ablog-cloud {list-style: none; overflow: auto;}
    ul.ablog-cloud li {float: left; height: 20pt; line-height: 18pt; margin-right: 5px;}
    ul.ablog-cloud a {text-decoration: none; vertical-align: middle;}
    li.ablog-cloud-1{font-size: 80%;}
    li.ablog-cloud-2{font-size: 95%;}
    li.ablog-cloud-3{font-size: 110%;}
    li.ablog-cloud-4{font-size: 125%;}
    li.ablog-cloud-5{font-size: 140%;}
  </style>
  <ul class="ablog-cloud">
    
      
      <li class="ablog-cloud ablog-cloud-1">
        <a href="../../blog/tag/gcn/">GCN</a></li>
      
    
      
      <li class="ablog-cloud ablog-cloud-5">
        <a href="../../blog/tag/icpc/">ICPC</a></li>
      
    
      
      <li class="ablog-cloud ablog-cloud-1">
        <a href="../../blog/tag/number-theory/">Number theory</a></li>
      
    
      
      <li class="ablog-cloud ablog-cloud-1">
        <a href="../../blog/tag/文献导读/">文献导读</a></li>
      
    
      
      <li class="ablog-cloud ablog-cloud-1">
        <a href="../../blog/tag/知识库/">知识库</a></li>
      
    
      
      <li class="ablog-cloud ablog-cloud-1">
        <a href="../../blog/tag/视觉问答/">视觉问答</a></li>
      
    
  </ul>

  <h3><a href="../../blog/archive/">归档</a></h3>
  <ul>
  
    
    <li><a href="../../blog/2019/">2019 (5)</a></li>
    
  
  </ul>

<div id="searchbox" style="display: none" role="search">
  <h3>快速搜索</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search/" method="get">
      <input type="text" name="q" />
      <input type="submit" value="转向" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, φKN1GHT.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.8.5</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../../_sources/articles/graph_convolutional_networks_with_mxnet_in_python.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>